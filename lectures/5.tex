%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Lecture 5
%            : University of Southern Maine 
%            : @james.quinlan
%            : Courtney Jackson
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\section*{Objectives}
\begin{outline}
    \1 Continuation of Prior Lecture
    \1 More about kNN
    \1 Code in Julia
    \1 XPL's
\end{outline}

\rule[0.0051in]{\textwidth}{0.00025in}
% ----------------------------------------------------------------
\section{Continuation of Prior Lecture}

Consider the following:

\begin{enumerate}
    \item Given $[a,b] \rightarrow [0,1]$ our objective is to map $a \rightarrow 0$ and $b \rightarrow 1$

    \[
        \begin{array}{rl}
            x &= \dfrac{1}{b-a}(x-a) \\\\
            &= \dfrac{1}{b-a}x - \dfrac{a}{b-a}
            \hspace{10em}
            \quad
            % Interval diagram
            \begin{tikzpicture}[baseline]
                % Draw the line
                \draw[-] (0,0) -- (4,0);
                % Draw the 3 ticks
                \foreach \i in {0,2,4} {
                    \draw[thick] (\i,0.1) -- (\i,-0.1);
                }
                % Draw the values under the ticks
                \node at (0,0) [below] {$a$};
                \node at (2,0) [below] {$\frac{a+b}{2}$};
                \node at (4,0) [below] {$b$};
            \end{tikzpicture} \\

            &= \dfrac{1}{b-a}\left(\dfrac{a+b}{2}\right) - \dfrac{2a}{2(b-a)} \\\\
            &= \dfrac{a+b - 2a}{2(b-a)} \\\\
            &= \dfrac{b-a}{2(b-a)} \\\\
            &= \dfrac{1}{2}
        \end{array}
    \]

    \begin{itemize}
        \item Check the formula by testing with $0,\frac{1}{2},1$, which make up the bounds and the midpoint. 
        \item It is given that $(a,0) \land (b,1)$ where $m=\frac{1-0}{b-a}=\frac{1}{b-a}$.
        \item We can then derive $y-0=\frac{1}{b-a}(x-a)= {\frac{1}{b-a}x-\frac{a}{b-a}}$
    \end{itemize}
    
    \item $[a,b] \rightarrow [c,d]$

    
    

    \begin{align*}
       \frac{x-a}{b-a}d + \frac{b-x}{b-a}c \\
       (a,c) \land (b,d) \rightarrow m = \frac{d-c}{b-a}
    \end{align*}
\end{enumerate}
% ----------------------------------------------------------------
\section{More about kNN \cite{guo2003knn}}

\begin{equation}
    \arg\max_{j=1,2,\ldots,m} \sum_{i \in N_k} I(c_j(y_i))
\end{equation}
where:
\begin{outline}
    \1 $j$ indexes the classes,
    \1 $k$ is the number of neighbors,
    \1 $N_k$ is the set of indices of the $k$ nearest neighbors,
    \1 $I(\cdot)$ is the indicator function.
\end{outline}

Suppose $j=2$, $k=8$, and it is a binary classification problem.
 

Example problem: \\
$y=[1,-1,-1, \ 1, \ 1, \ 1,-1, \ 1]$ \\
$c = [1, \ -1$], \\
$x_t \rightarrow 1$ \\
$\arg\max_{j=[1,2]}, \ c_j=1, $ \\$[\mathcal{I}(1=1)+\mathcal{I}(1=-1)+\mathcal{I}(1=-1)+\mathcal{I}(1=1)+\mathcal{I}(1=1)+\mathcal{I}(1=1)+\mathcal{I}(1=-1)+\mathcal{I}(1=1)],$ \\
$= 1+0+0+1+1+1+0+1 = 5$\\
$\arg\max_{j=[1,2]}, \ c_j=-1, $ \\
$[\mathcal{I}(-1=1)+\mathcal{I}(-1=-1)+\mathcal{I}(-1=-1)+\mathcal{I}(-1=1)+\mathcal{I}(-1=1)+\mathcal{I}(-1=1)+\mathcal{I}(-1=-1)+\mathcal{I}(-1=1)]$ \\
$= 0+1+1+0+0+0+1+0 = 3$\\
$\arg\max_{j=[1,2]}$ $[5,3]=1$ \\
When $j = 1$, the function returns $5$, versus it returning $3$ when $j = 2$,
so the \textit{argument} which leads to the \textit{maximum} output is
1.

% ----------------------------------------------------------------

\section{Code in Julia}
The following is code written in Julia 1.10.4. 
It is represeting a kNN algorithm on the Iris dataset.

\begin{lstlisting}
using Pkg
using Distances
using RDataSets
using MLJBase
Pkg.add("StatsBase")
using StatsBase

iris=dataset("datasets","Iris")
x = Matrix(iris[:,1:4])
y=@.ifelse(iris.species=="setosa",1,-1)
c = unique(y)
c[argmax(map(i->sum(y.==c[i]),1:lastindex(c)))]
mode(y)

#This can also be done using the NearestNeighbors package
function kNN(X,x,y,k,d = Euclidean())
    n-size(X,2)
    distances = map(i-> d(x,X[:,i]), 1:n)
    indices = partialsortperm(distances,1:k)
    if typeof(y) == Vector{union{Float32,Float64}}
        yhat = mean(y[indices])
    else
        yhat = mode
    end
    return yhat
end

#Metrics
accuracy = sum(yhat.==ytest/length(ytest))
precsion = sum ((yhat.==1.&(ytest.==1)))/sum(yhat.==1)
recall = sum((yhat.==1.&(ytest.==1)))/sum(ytest.==1)
specircity = sum((yhat.==-1).&(ytest.==-1))/sum(ytest.==-1)
f1 = 2*[precsion * recall / (precision + recall)
Distances.Hamming()(ytest,yhat)
train, test = partition(1:size(X,2),0.7,shuffle=true)

    Xtrain = X[:,train]
    Xtest = X[:test]
    ytrain = y[train]
    ytest = y[test]

yhat = map(i->kNN(Xtrain,Xtest[:,i],ytrain,1)1:size(Xtest,2))

\end{lstlisting}

The following is some code about setting a random seed
\begin{lstlisting}
Pkg.add("random")
using Random
Random.seed!(123) #Where 123 is the seed
\end{lstlisting}

% ----------------------------------------------------------------

\section{XPL's}
1.) Code up a partition function

\bibliographystyle{plain}
\bibliography{references}
