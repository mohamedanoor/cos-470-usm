%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Lecture 11
%            : Optimization & SVMs -- Deriving the Loss function
%            : University of Southern Maine 
%            : @james.quinlan
%            : @abdirahman.mohamed, Wyatt McCurdy
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\section*{Objectives}
\begin{outline}
    \1 Understand monotonic functions and their properties.
    \1 Learn the formulation of SVMs through maximum margin optimization.
    \1 Study Lagrange multipliers in constrained optimization.
    \1 Explore gradients, partial derivatives, and various gradient descent methods.
    \1 Understand the primal problem (objective function) of SVMs.
    \1 Understand the dual problem using Lagrange multipliers.
    \1 Practice computing the gradient.
\end{outline}

\rule[0.0051in]{\textwidth}{0.00025in}
% ----------------------------------------------------------------

\section{Mathematics and SVM Optimization}

\subsection{Monotonic Functions}
\begin{itemize}
    \item \textbf{Definition:} A function \( f(x) \) is \emph{monotonic} if whenever 
    \[
    x_1 \leq x_2, \quad \text{then} \quad f(x_1) \leq f(x_2).
    \]
    \item \textbf{Takeaway:} A monotonic function preserves the order of inputs — as one input increases, the output either increases or stays the same (non-decreasing), or decreases or stays the same (non-increasing).
\end{itemize}

\subsection{Maximum Margin in SVMs}
\begin{itemize}
    \item \textbf{Margin:} The gap between the decision boundary and the closest data points (support vectors).
    \item \textbf{Goal:} Maximize the margin, which is equivalent to maximizing 
    \[
    \frac{1}{\|w\|}.
    \]
    \item \textbf{Alternate Forms:} Minimizing any of these differentiable forms is equivalent:
    \begin{itemize}
        \item \(\|w\|\)
        \item \(\|w\|^2\)
        \item \(\frac{1}{2}\|w\|^2\)
    \end{itemize}
\end{itemize}

\subsection{SVM Optimization Setup (Primal Problem)}
\begin{itemize}
    \item \textbf{Objective:}
    \[
    \min \frac{1}{2}\|w\|^2.
    \]
    \item \textbf{Constraints:} For every sample \((x_i, y_i)\),
    \[
    y_i (w^T x_i + b) \geq 1,
    \]
    or equivalently,
    \[
    y_i (w^T x_i + b) - 1 \geq 0.
    \]
\end{itemize}

\subsection{Lagrange Multipliers and the Dual Problem}
\begin{itemize}
    \item \textbf{Purpose:} Incorporate inequality constraints into the optimization problem.
    \item \textbf{Method:} Multiply each constraint by a Lagrange multiplier \(\alpha_i \geq 0\):
    \[
    \alpha_i \bigl( y_i (w^T x_i + b) - 1 \bigr).
    \]
    \item \textbf{Lagrangian Formulation:}
    \[
    L(w, b, \alpha) = \frac{1}{2}\|w\|^2 - \sum_{i=1}^{n} \alpha_i \bigl( y_i (w^T x_i + b) - 1 \bigr).
    \]
    \item \textbf{Dual Problem:} By taking derivatives with respect to \(w\) and \(b\) and setting them to zero, we obtain:
    \[
    w = \sum_{i=1}^{n} \alpha_i y_i x_i \quad \text{and} \quad \sum_{i=1}^{n} \alpha_i y_i = 0.
    \]
    Substituting \(w\) back into the loss function leads to the dual formulation:
    % Dual objective: maximize this quadratic function to find optimal α
    \[
    \max_{\alpha} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j x_i^T x_j.
    \]
\end{itemize}

\section{Computing the Gradient}

The loss function for SVMs is given by:
\[
L = \frac{1}{2}\|w\|^2 - \sum_{i=1}^{n} \alpha_i \bigl( y_i (w^T x_i + b) - 1 \bigr).
\]

\subsection{The Left-hand Derivative}
The derivative of \(\frac{1}{2}\|w\|^2\) with respect to \(w\) is:
\[
\frac{\partial \left(\frac{1}{2}\|w\|^2\right)}{\partial w} = w.
\]

\subsection{The Right-hand Derivative}
For the term 
\[
\sum_{i=1}^{n} \alpha_i (y_i(w^T x_i + b) - 1),
\]
the derivative with respect to \(w\) is:
\[
\frac{\partial}{\partial w} \sum_{i=1}^{n} \alpha_i y_i w^T x_i = \sum_{i=1}^{n} \alpha_i y_i x_i.
\]
Similarly, the derivative with respect to \(b\) is:
\[
\frac{\partial}{\partial b} \sum_{i=1}^{n} \alpha_i y_i b = \sum_{i=1}^{n} \alpha_i y_i.
\]

Setting these derivatives equal to zero yields:
\[
w = \sum_{i=1}^{n} \alpha_i y_i x_i \quad \text{and} \quad \sum_{i=1}^{n} \alpha_i y_i = 0.
\]

Substituting \(w\) back into the loss function gives:
\[
L = \sum_{i=1}^{n} \alpha_i - \frac{1}{2}\, w^T w,
\]
which, after replacing \(w\) with \(\sum_{i=1}^{n} \alpha_i y_i x_i\), becomes:
\[
L = \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j x_i^T x_j.
\]

\begin{tcolorbox}
\textbf{XPL1}: Demonstrate that 
\[
\sum_{i=1}^n \alpha_i \sum_{j=1}^n \beta_j = \sum_{i=1}^n \sum_{j=1}^n \alpha_i\beta_j.
\]
\end{tcolorbox}

\begin{tcolorbox}
\textbf{Answer to XPL1}: 

We expand the left-hand side:
\[
\sum_{i=1}^n \alpha_i \sum_{j=1}^n \beta_j = \sum_{i=1}^n \left( \sum_{j=1}^n \alpha_i \beta_j \right).
\]
Since \(\alpha_i\) is constant with respect to the inner sum over \(j\), it distributes:
\[
= \sum_{i=1}^n \sum_{j=1}^n \alpha_i \beta_j.
\]
Thus, the identity holds.
\end{tcolorbox}

\section{Summary}
In these notes, we have:
\begin{itemize}
    \item Reviewed monotonic functions and the concept of maximum margin in SVMs.
    \item Set up the primal optimization problem for SVMs.
    \item Incorporated constraints using Lagrange multipliers to derive the dual problem.
    \item Computed gradients with respect to the weight vector \(w\) and bias \(b\) to establish optimality conditions.
\end{itemize}

This framework underpins many machine learning optimization techniques, including various gradient descent methods used in practice.
