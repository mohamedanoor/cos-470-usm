%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Lecture 15 
%            : Optimizers
%            : University of Southern Maine 
%            : @james.quinlan
%            : Gabrielle Akers - Lecture 15
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{outlines}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{hyperref}

\usetikzlibrary{positioning, arrows.meta, shapes}

\title{Optimizers in Machine Learning}
\author{University of Southern Maine}
\date{Lecture 15}


\begin{document}

\maketitle

\section*{Objectives}
\begin{outline}
    \1 Optimizers
    \1 Batch Gradient Descent
    \1 Momentum
    \1 RMSProp
\end{outline}

\rule[0.0051in]{\textwidth}{0.00025in}
% ----------------------------------------------------------------
\section{Optimizers}
\begin{enumerate}
    \item Batch Gradient Descent
    \begin{itemize}
        \item In the past couple of classes we have been focusing primarily on Batch Gradient Descent and the math behind it.
    \end{itemize} 
    \textbf{Issues with Batch Gradient Descent}
    \begin{enumerate}
        \item Computationally expensive
        \begin{itemize}
            \item Batch gradient descent computes the gradient at every single data point, which ensures points don't get passed over, but also requires a lot of needless computations
        \end{itemize} 
        \item Notational issues
        \begin{align}
            \text{Individual loss:} & \; \ell(w,x) \\
            \text{Total loss:} & \; L(w,X) = \frac{1}{n} \sum_{i=1}^n \ell(w,x_i) \\
            \text{Gradient:} & \; \nabla_w L(w,X) = \nabla_w \left(\frac{1}{n} \sum_{i=1}^n \ell(w,x_i)\right) = \frac{1}{n} \sum_{i=1}^n \nabla_w \ell(w,x_i)
        \end{align}
        
        The derivative of a sum should be equal to the sum of the derivatives
        \item Slow convergence
        \begin{itemize}
            \item The steps get progressively smaller
        \end{itemize}
        \item Gets stuck on local minimums
    \end{enumerate}
    
    \textbf{When to use Batch Gradient Descent:}
    \begin{itemize}
        \item Use for small to medium-sized datasets that fit in memory
        \item For simple models like linear regression or logistic regression
    \end{itemize}
    
    \textbf{Related Types of Gradient Descent}
    \begin{enumerate}
        \item Stochastic Gradient Descent (SGD)
        \begin{itemize}
            \item This type of gradient descent selects a random x every time
            \begin{align}
                \ell(w,x_r)
            \end{align}
        \end{itemize}
        
        \textbf{When to use SGD:}
        \begin{itemize}
            \item For very large datasets where batch processing is impractical
            \item When training with streaming or online data
        \end{itemize}

        \item Minibatch Gradient Descent
        \begin{itemize}
            \item A combination of SGD and Batch Gradient Descent. It uses a sample of x values and selects a random value from that sample for computation.
            \item The general rule for the batch size is 32 or less.
        \end{itemize} 
        \begin{align}
            \text{batch size} \leq 32
        \end{align}
        
        \textbf{When to use Minibatch Gradient Descent:}
        \begin{itemize}
            \item Most common approach for deep learning applications
            \item When training on GPUs 
            \item For neural networks with many parameters
        \end{itemize}
    \end{enumerate}
    
    \item Momentum
    \begin{itemize}
        \item A type of gradient descent that takes into consideration previous gradients in order to prevent getting stuck at local minimums.
        \begin{align}
            w_{t+1} = w_t - \alpha \nabla_w L(w_t) - \text{previous gradients}
        \end{align}
    \end{itemize}
    \textbf{Features of Momentum}
    \begin{enumerate}
        \item Faster convergence than Batch Gradient Descent
        \item Doesn't get stuck at local minimums
        \item The amount of past gradients used are controlled with exponentially decaying weighted averages
        \begin{align}
            v_{t+1} &= \beta v_t + (1-\beta)\nabla_w L(w_t) \\
            w_{t+1} &= w_t - \alpha v_{t+1}
        \end{align}
        
        Hyperparameters:
        \begin{align}
            \alpha &= \text{learning rate} \\
            \beta &= \text{weights}
        \end{align}
        
        \item Hyperparameters are tuned with cross validation, as shown in Figure \ref{fig:data_split_extended}, which was originally created in Lecture 6.
        % \begin{align}
 \begin{figure}[ht]
    \centering
    \begin{tikzpicture}[node distance=1.5cm and 2cm, auto]
        % Nodes
        \node (data) [draw, rectangle] {Data};
        \node (train) [below left=of data, draw, rectangle] {Training Set};
        \node (test)  [below right=of data, draw, rectangle] {Test Set};
        \node (dev)   [below left=of train, draw, rectangle] {Dev Set};
        \node (val)   [below right=of train, draw, rectangle] {Validation Set};
        \node (build) [below=of dev, draw, rectangle] {Develop, build, or train model};
        \node (train_all) [below=of val, draw, rectangle] {Train on everything};
        \node (final_test) [below=of train_all, draw, rectangle] {Finally test/retrain if test was good};
        
        % Arrows
        \draw[->] (data) -- (train);
        \draw[->] (data) -- (test);
        \draw[->] (train) -- (dev);
        \draw[->] (train) -- (val);
        \draw[->] (dev) -- (build);
        \draw[->] (val) -- (train_all);
        \draw[->] (build) -- (train_all);
        \draw[->] (train_all) -- (final_test);
        \draw[->, bend left=45] (test) to (final_test);
    \end{tikzpicture}
    \caption{Lecture 6 - Model Training Process}
    \label{fig:data_split_extended}
\end{figure}
        % \end{align}
        \begin{itemize}
            \item The best practice has $\beta = 0.9$, but it just has to be less than 1
        \end{itemize} 
        \item Update function
        \begin{align}
            w_{t+1} = \sum_{i=0}^{t} \beta^i(1-\beta)\nabla L_{t-i}
        \end{align}
    \end{enumerate}
    
    \textbf{When to use Momentum:}
    \begin{itemize}
        \item For training deep neural networks with many layers
        \item When the loss landscape has steep ravines or narrow valleys
        \item When training is slow or oscillating with standard gradient descent
    \end{itemize}
    
    \textbf{Issues with Momentum}
    \begin{enumerate}
        \item Can overshoot and miss the global minimum
        \item Still can get stuck at some local minimums
    \end{enumerate}
    
    Unrolling the recurrence relation:
    \begin{align}
        v_{t+1} &= \beta v_t + (1-\beta)\nabla L_t \\
        v_{t+1} &= \beta (\beta v_{t-1} + (1-\beta)\nabla L_{t-1}) + (1-\beta)\nabla L_t \\
        v_{t+1} &= \beta^2 v_{t-1} + \beta(1-\beta)\nabla L_{t-1} + (1-\beta)\nabla L_t \\
        v_{t+1} &= \beta^2(\beta v_{t-2} + (1-\beta)\nabla L_{t-2}) + \beta(1-\beta)\nabla L_{t-1} + (1-\beta)\nabla L_t \\
        v_{t+1} &= \beta^3 v_{t-2} + \beta^2(1-\beta)\nabla L_{t-2} + \beta(1-\beta)\nabla L_{t-1} + (1-\beta)\nabla L_t
    \end{align}
    
    \item RMSProp
    \begin{itemize}
        \item A type of gradient descent that is adaptable depending on the size of the gradient
    \end{itemize}
    \begin{align}
        \nabla L(w) &= \begin{bmatrix}
               \frac{\partial L}{\partial w_1}    \\
               \frac{\partial L}{\partial w_2}    \\
               \vdots   \\
               \frac{\partial L}{\partial w_n}    \\
             \end{bmatrix}
    \end{align}
    
    \textbf{When to use RMSProp:}
    \begin{itemize}
        \item For training recurrent neural networks (RNNs and LSTMs)
        \item When different parameters require different learning rates
    \end{itemize}
    
    \textbf{Features of RMSProp}
    \begin{enumerate}
        \item Weighs gradients differently in different directions
        \begin{enumerate}
            \item Large gradient $\rightarrow$ Small step
            \item Small gradient $\rightarrow$ Large step
        \end{enumerate}
        \item Learning rates are different for each component
        \item Element wise operations
        \begin{align}
            u \times v &\rightarrow \text{cross product} \rightarrow \text{vector} \\
            u \cdot v &\rightarrow \text{dot product} \rightarrow \text{scalar} \\
            u \odot v &\rightarrow \text{Hadamard product} \rightarrow \text{vector}
        \end{align}
    \end{enumerate}
    
    \item AdaGrad
    
    \textbf{When to use AdaGrad:}
    \begin{itemize}
        \item For natural language processing tasks
        \item For relatively simple models with convex loss functions
    \end{itemize}
    
    \item NAG (Nestrov Accelerated Gradient)
    
    \textbf{When to use NAG:}
    \begin{itemize}
        \item For problems where precise control of optimization is needed
        \item When training convolutional neural networks
    \end{itemize}
    
    \item Adam
    \begin{itemize}
        \item Created in 2015, Adam is generally considered to be the best type of gradient descent. It takes the best features of both Momentum and RMSProp and combines them into a single type of gradient descent.
    \end{itemize}
    
    \textbf{When to use Adam:}
    \begin{itemize}
        \item For most deep learning applications as a good default choice
        \item When training complex models like GANs or transformers
        \item When you don't have time to tune optimizer hyperparameters
        \item For problems with noisy or sparse gradients
    \end{itemize}
\end{enumerate}
\end{document}
